---
title: "Ref-AVS"
layout: default
---

<!-- <div style="position: fixed; bottom: 15px; right:1px;">
  <a href=""> <img src="{{ site.baseurl }}/static/img/logo/cn.png" width="50%"; /> </a>
</div> -->

<!-- About -->
<section class="bg-light" id="about">
  <div class="container">

    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">Update</h2>
        <h5 class="text-muted" style="text-align:left;">
          <ul>
              <li>? ? 2022: The dataset has been uploaded to <a href="https://drive.google.com/placeholder"><b>Google Drive</b></a>, welcome to download and use!</li>
              <li>? ? 2022: The Ref-AVS dataset has been released, please see <font color="danger">Download</font> for details.</li>
              <li>? ? 2022: Code has been released <a href="https://github.com/GeWu-Lab/Ref-AVS">here</a>!</li>
              <li>? ? 2022: Watch the project's video demonstration on 
                <a href="https://www.youtube.com/placeholder">YouTube</a> or 
                <a href="https://www.bilibili.com/placeholder">Bilibili</a>.</li>
              <li>15 July 2024: Camera-ready version has been released <a href="{{ site.baseurl }}/static/files/placeholder.pdf">here</a>!</li>
              <li>01 July 2024: Our paper is accepted for publication at ECCV2024. Camera-ready version and code will be released soon!</li>
          </ul>
        </h5>
      </div>  
    </div>

    <br/>

    
    <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase">What is Ref-AVS task?</h3>
          <p  class="text-muted">
            Traditional reference segmentation tasks have predominantly focused on silent visual scenes, neglecting the integral role of multimodal perception and interaction in human experiences. In this work, we introduce a novel task called Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment objects within the visual domain based on expressions containing multimodal cues. 
            Such expressions are articulated in natural language forms but are enriched with multimodal cues, including audio and visual descriptions.
          </p>
          <center><img src="{{ site.baseurl }}/static/img/refavs/teaser.png" alt="" style="width:70%;  margin-top:8px; margin-bottom:15px;"></center>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12">
        <!-- <ul> -->
          <p class="text-muted">
            <b>For instance</b>, as shown in this Figure, Ref-AVS challenges machines to locate objects of interest in the visual space using multimodal cues, just like humans do in the real world, comparing to the Ref-AVS task with other related tasks..
          </p>
        <!-- </ul> -->
      </div>
    </div>

    <br/>

    <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase">What is Ref-AVS dataset?</h3>
        <!-- <h5 class="text-muted" style="text-align:left;"> -->
          <!-- <ul> -->
            <p class="text-muted">
              To facilitate this research, we construct the first Ref-AVS benchmark, which provides pixel-level annotations for objects described in corresponding multimodal-cue expressions. 
            </p>
            <!-- <p class="text-muted">
              <b>Why musical performance? </b>Considering that musical performance is a typical multimodal scene consisting of abundant audio and visual components as well as their interaction, it is appropriate to be utilized for the exploration of effective audio-visual scene understanding and reasoning.
            </p> -->

          <!-- </ul> -->
        </h5>
      </div>
    </div>



    <div class="row">
      <!-- news column -->
      <div class="col-md-4">
        <h4 class="service-heading">Basic informations</h4>
        <p class="text-muted">We choose to manually collect videos from YouTube. Specifically, 20 categories of musical instruments, 8 of animals, 15 of machines, and 5 of humans. Annotations are collected using our customized GSAI-Labeled system.</p>

      </div>
      <!-- characteristics column -->
      <div class="col-md-4">
        <h4 class="service-heading">Characteristics</h4>
        <ul class="text-muted">
          <li class="text-muted">20,261 expressions</li>
          <li class="text-muted">48 categories</li>
          <li class="text-muted">4,002 videos</li>
          <li class="text-muted">40,020 frames</li>
          <li class="text-muted">6,888 objects</li>
          <li class="text-muted">Audio, visual, temporal</li>
          <li class="text-muted">Pixel-level annotation</li>
          <li class="text-muted">Diversity, complexity and dynamic</li>
        </ul>
      </div>

      <!-- udated column -->
      <div class="col-md-4">
        <h4 class="service-heading">Personal data/Human subjects</h4>
        <!-- <ul class="text-muted"> -->
          <p class="text-muted">Videos and Frames in Ref-AVS are public on YouTube, and annotated via crowdsourcing. We have explained how the data would be used to crowdworkers. Our dataset does not contain personally identifiable information or offensive content.</p>
        <!-- </ul> -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <video autoplay muted loop width="100%">
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm"> -->
            <source src="{{ site.baseurl }}/static/videos/music_avqa_720p.mp4" type="video/mp4">
              <source src="{{ site.baseurl }}/static/videos/music_avqa_720p.mp4" type="video/mp4">
                Sorry, we cannot display the MUSIC-AVQA video wall as your browser doesn't support HTML5 video.
        </video>
      </div>
    </div>
    <!-- video banner row -->
      
       


  </div>
</section>

<!-- Stats -->
<section id="stats">
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading text-uppercase">Ref-AVS Dataset</h2>
        <h3 class="section-subheading text-muted">Some graphical representations of our dataset and annotations</h3>
      </div>
    </div>



      
    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/refavs/dataset_case.png" style="width: 100%" class="img-responsive"/> 
      </div>
    </div>
    <p class="text-muted" style="text-align:left">
      <b>Illustrations of our MUSIC-AVQA dataset examples</b>.
    </p>


    <!-- <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/matrix.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left"><b>Left: </b>Number of combinations of different types of instruments, where the lighter the color, the more the number. And instruments outside the 22 instrument categories are denoted by other. The confusion matrix shows that the combination of different instruments is diversified. <b>Right-top: </b> According to Wikipedia, 22 kinds of instruments are divided into 4 categories: <i>String, Wind, Percussion</i> and <i>Keyboard</i>. <b>Right-bottom: </b>9 question types in different scenarios.</p>
    </div> -->


    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/refavs/feature.png" style="width: 50%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left">
        <b>The distribution for both expressions and objects</b>. It visualizes the co-occurrence of objects in our dataset, where we can observe a dense web of connections spanning various categories, such as musical instruments, people, vehicles, etc. The rich combination of categories indicates that our dataset is not limited to a narrow set of scenarios but rather encompasses a broad spectrum of real-life scenes where such objects are likely to naturally appear together</b>.
      </p>
    </div>

    <br/>


    <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" id="downloadFiles" style="text-align:left; margin-left:-14px">How was Ref-AVS dataset made?</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        We design an audio-visual customized system to collect question and segmentation label, and all information are collected with this system. The flow chart of the labeling system is shown in below figure.
      </p>
      <center>
        <img src="{{ site.baseurl }}/static/img/stats-figures/annotation_framework.png" alt="" style="width:88%;  margin-top:10px; margin-bottom:10px;"> 
      </center>
      <p class="text-muted" style="text-align:left">
        Dataset collection pipeline. From the beginning, this pipeline has played a significant role in ensuring the efficiency and cost-effectiveness of the overall process, leading to the successful acquisition of high-quality samples.
      </p>
    </div>

    <br/>


    <!-- <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-left:-14px">QA pairs samples</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        <b>Demo.</b> The large-scale spatial-temporal audio-visual dataset that focuses on question-answering task, as shown in below figure that different audio-visual scene types and their annotated QA pairs in the AVQA dataset.
      </p>

      <hr/>

      <div class="col-md centered" style="padding:0.3rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/st_avqa_pairs.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted", style="text-align:left">
        In the first row, <b>a)</b>, <b>b)</b>, and <b>c)</b> represent real musical performance videos, namely solo, ensemble of the same instrument, and ensemble of different instruments. In the second row, <b>d)</b>, <b>e)</b>, and <b>f)</b> represent the synthetic video, which are audio and video random matching, audio overlay, and video stitching, respectively.
      </p>
    </div> -->


    <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-left:-14px">More video examples</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        Some video examples with QA pairs in the MUSIC-AVQA dataset. 
        Through these examples, we can have a better understanding of the dataset, and can more intuitively feel the QA tasks in dynamic and complex audio-visual scenes
      </p>
      <p class="text-muted" style="text-align:left; margin-right: 10px;">
 
        <!-- example 1 -->
        <table>
          <tr>
            <td style="width:1%"></td>
            <td style="width:31%">
              <video width="100%" controls="controls">
                <source src="{{ site.baseurl }}/static/videos/clip1.mp4" align="left" type="video/mp4">
              </video>
            </td>
            <!-- <td style="width:1%"></td> -->
            <!-- <td style="width: 67%; text-align:left; padding:0.2rem" class="text-muted">
              <b>Question</b>: How many instruments are sounding in the video? <br>
              <b>Answer</b>: two <br>
              To answer the question, an AVQA model needs to first identify objects and sound sources in the video, and then count all sounding objects. Although there are three different sound sources in the audio modality, only two of them are visible. Rather than simply counting all audio and visual instances, exploiting audio-visual association is important for AVQA.
            </td> -->
          </tr>
        </table>

        <!-- example 2 -->
        <table>
          <tr>
            <td style="width:1%"></td>
            <td style="width:31%">
              <video width="100%" controls="controls">
                <source src="{{ site.baseurl }}/static/videos/clip2.mp4" align="left" type="video/mp4">
              </video>
            </td>
            <!-- <td style="width:1%"></td>
            <td style="width: 67%; text-align:left; padding:0.2rem" class="text-muted">
              <b>Question</b>: What is the first sounding instrument? <br>
              <b>Answer</b>: piano <br>
              To answer the question, an AVQA model needs to not only associate all instruments and their corresponding sounds in the video, but also identify the first instrument that makes sounds. Thus, the AVQA task is not a simple recognition problem, and it also involves <font color="orange"><b>audio-visual association</b></font> and <font color="orange"><b>temporal reasoning</b></font>. 

            </td> -->
          </tr>
        </table>

        <!-- example 3 -->
        <table>
          <tr>
            <td style="width:1%"></td>
            <td style="width:31%">
              <video width="100%" controls="controls">
                <source src="{{ site.baseurl }}/static/videos/clip3.mp4" align="left" type="video/mp4">
              </video>
            </td>
            <!-- <td style="width:1%"></td>
            <td style="width: 67%; text-align:left; padding:0.2rem" class="text-muted">
              <b>Question</b>: What is the left instrument of the second sounding instrument? <br>
              <b>Answer</b>: guzheng <br>
              To answer the question, an AVQA model needs to first identify the second sounding instrument: flute and then infer the instrument at its left. Besides recognizing objects, exploring audio-visual association, and performing temporal reasoning, <font color="orange"><b>spatial reasoning</b></font> is also crucial for AVQA.
            </td> -->
          </tr>
        </table>

        
      </p>

    </div>


<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" id="downloadFiles">QA pairs samples</h4>
      </div>
      <hr/> -->
    <!--   <div class="text-muted" style="text-align:left">
      
      </div> -->
<!--       <hr/>
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/st_avqa_pairs.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left">
        The large-scale spatial-temporal audio-visual dataset that focuses on question-answering task, as shown in:
      </p>
    </div> -->

<!--     <div class="row justify-content-md-center text-center">
    <div class="col-md-12">
      <h4 class="section-subheading" id="downloadFiles">How was MUSIC-AVQA dataset made?</h4>
        <div class="col-md centered" style="padding:1rem;">
          <img src="{{ site.baseurl }}/static/img/stats-figures/annotation_framework.png" style="width: 100%" class="img-responsive"/> 
        </div>
        <p>
          <b>Flow chart of the labeling system.</b> Labeling system contains questioning and answering. In the questioning section, the annotator is required to select the performance type of the video and the included instruments, and then scene types, question types, and question templates, and finally one question is automatically generated based on the previous selection. In the answering part, the annotator to judge whether the question is reasonable, and if it is unreasonable, the question will be labeled again. Then, the annotator answering the question according to video content, and finally one QA pair is produced.
        </p>
        <hr/>
    </div>
    </div> -->


    <!-- <br/><br/> -->
<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat2.png" style="width: 100%" class="img-responsive"/> 
      </div>
    </div>

    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat3.png" style="width: 100%" class="img-responsive"/> 
        <h4>Distribution</h4>
      </div>
    </div> -->
      


      <!-- <div class="col-md-6 centered" style="padding:1rem; vertical-align:bottom"> -->
        <!-- TO ADD GRAPH: replace div below, ex: above <img> tag -->
        <!-- <img src="{{ site.baseurl }}/static/img/stats-figures/masks.png" style="width: 100%" class="img-responsive"/>   -->
        <!-- <h4>Automatic Annotations</h4> -->
      <!-- </div> -->
    </div>

    <!-- <div class="row justify-content-md-center text-center">
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph4" style="width: 100%" class="img-responsive"></div>
      <h4>Resolution</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph5" style="width: 100%" class="img-responsive"></div>
      <h4>Number of Frames</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph6" style="width: 100%" class="img-responsive"></div>
      <h4>Total number of hours</h4>
      </div>
      </div>
      <div class="row justify-content-md-center text-center">
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph7" style="width: 100%" class="img-responsive"></div>
      <h4>Number of annotators<br/>used per video</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph8" style="width: 100%" class="img-responsive"></div>
      <h4>Splits</h4>
      </div>
      </div> -->

      <!-- <div class="col-md-6">
        <div class="card" style="border: solid 2px; background-color: #373435ff; margin-bottom:5px;">
        <h1 style=" color: white; text-decoration: underline; text-decoration-color: #ed323eff;"> Baseline Models </h1>
        <div id="graph9"></div>
        </div>

        <div class="card" style="border: solid 2px; background-color: #ed323eff;">
        <h1 style=" color: white; text-decoration: underline; text-decoration-color: #373435ff;"> State of the Art Results </h1>
        <div id="graph10"></div>
        </div>

        </div> -->
  </div>
</section>




<section class="bg-light" id="downloads">
  <div class="container">
    
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Download</h2>
        <h3 class="section-subheading text-muted">Dataset publicly available for research purposes</h3>
      </div>
    </div>


    <div class="row">
      <div class="col-md-12"> 
        
        <h4 class="section-subheading" id="downloadFiles">Data and Download </h4><hr/>
        <P>Raw videos:
          <ul>
            <li>
              <a href="https://drive.google.com/drive/folders/1WAryZZE0srLIZG8VHl22uZ3tpbGHtsrQ?usp=sharing">Google Drive</a>
            </li>
            <li>Baidu Drive (<b>password: cvpr</b>)</li>
            - <a href="https://pan.baidu.com/s/1yVPfOXyDesHdUZFHK3tYog">Real videos</a> (36.67GB) <br/>
            - <a href="https://pan.baidu.com/s/1b7HQbMdcfaWjsHdWiLWO2Q">Synthetic videos</a> (11.59GB) <br/>
            <b>Note</b>: Please move all downloaded videos to a folder, for example, create a new folder named MUSIC-AVQA-Videos, which contains 9,288 real videos and synthetic videos.
          </ul> 
        </P>
          
        <p>Raw videos frames (1fps): Available at <a href="https://pan.baidu.com/s/1c9gvJrf6oGXqHVtNiuOlZQ">Baidu Drive</a> (14.84GB) (<b>pwd: cvpr</b>) or <a href="">Google Drive</a> (coming soon!). In fact, we thought it might be more convenient to execute the code above to extract video frames.</p>
        
        <p>Features (VGGish, ResNet18 and R(2+1)D):
          <ul>
            <li>VGGish feature shape: [T, 128]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://pan.baidu.com/s/1TWzuXVPncuGFIv37q5rtKg">Baidu Drive</a> (pwd: cvpr) or 
              <a href="https://drive.google.com/file/d/1n6mEKdjA5nqAHI89HDPoxNELRa8ayIuN/view?usp=sharing">Google Drive</a>,&nbsp;(~112.7M)</li>
            <li>ResNet18 feature shape: [T, 512]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://pan.baidu.com/s/1o-QSe0HJymeXAegVRA3bPw">Baidu Drive</a> (pwd: cvpr) or&nbsp;
              <a href="https://drive.google.com/file/d/1mDZRvGMJIFyYytV_1ZLsolIFbAnoUxuC/view?usp=sharing">Google Drive</a>,&nbsp;(~972.6M)</li></li>
            <li>R(2+1)D feature shape: [T, 512]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://pan.baidu.com/s/13Ml-Je3Mmu46OSuMfYc6vQ">Baidu Drive</a> (pwd: cvpr) or 
              <a href="https://drive.google.com/file/d/1q3benITu0dgL6HEx1c0MX-alcZNMdd5N/view?usp=sharing">Google Drive</a>,&nbsp;(~973.9M)</li></li>
          </ul>
        </p>

        <p> Annotations (QA pairs, etc.): Available for download at <a href="https://github.com/GeWu-Lab/MUSIC-AVQA/tree/main/data/json">GitHub</a>.</p>          
        
        
        <br/>
        <h4 class="section-subheading">How to read the annotation files?</h4>
        <p>
          The annotation files are stored in JSON format. Each annotation file contains seven different keyword: "video_id", "question_id", "type", "question_content", "templ_values", "question_deleted" and "anser". 
          <!-- The information contained in each keyword is as follows: -->
          Below, we present a detailed explanation of each keyword.
          <ul class="text-muted">
            <li>"type": the question's modality information and type.</li>
            <li>"question_id": the unique identifier to QA pairs. .</li>
            <li>"video_id", "question_content", "templ_values" and "anser": The contents of these keywords together construct the Q-A pairs corresponding to the video with id "video_id". The form of &lt;<i>FL</i>&gt; is the template word in the question, and its specific content is the information contained in "templ_values". See the <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA.pdf">paper</a> for a more specific question template description</li>
            <li>"question_deleted": The check code during data annotation process and it will not be used in the dataloader.</li>
          </ul>
        </p>
        <p>
        Below, we show an example entry from the JSON file:
        </p>
        <pre class="sample-annotation">
        <code>
        {
          "video_id": "00000272",
          "question_id": 50,
          "type": "[\"Audio-Visual\", \"Temporal\"]",
          "question_content": "Where is the &lt;<i>FL</i>&gt; sounding instrument?",
          "templ_values": "[\"first\"]",
          "question_deleted": 0,
          "anser": "right"
        }</code>
        </pre>
        <p>
          The example shows the information and annotations related to the QA pairs corresponding to the video with id "video_id". As noted, we assign the unique identifier "50" to that QA pairs. 
          From the entry, we can retrieve the video name, questions, answer and type where the QA pairs belongs. 
          For example, the example above needs to <b>combine audio and visual modality information</b> to make a correct answer to this <i>temporal</i> question.
        </p>          

        <br/>
        <h4 class="section-subheading">Publication(s)</h4>
        <p>If you find our work useful in your research, please cite <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA.pdf">our CVPR 2022 paper</a>.
        </p>
        <pre class="bibtex" style="text-align:left; margin-left:8px">
        <code>
        @ARTICLE{Li2022Learning,
          title={Learning to Answer Questions in Dynamic Audio-Visual Scenarios},
          author={Guangyao li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, Di Hu},
          journal   = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
          year      = {2022},
          <!-- Url       = {https://doi.org/xxx} -->
        }</code>
        </pre>


        <br/>
        <h4 class="section-subheading">Disclaimer </h4>
        <!-- <p>The MUSIC-AVQA was collected as a tool for research in computer vision. The dataset may have unintended biases (including those of a societal, gender or racial nature).</p> -->
        <p> The released MUSIC-AVQA dataset is curated, which perhaps owns potential correlation between instrument and geographical area. This issue warrants further research and consideration.
        </p>

      </div>
    </div>

      
      <!-- <div class="row">
        <div class="col-md-12">
          <h4 class="section-subheading">Disclaimer </h4>
            <p>The MUSIC-AVQA was collected as a tool for research in computer vision. The dataset may have unintended biases (including those of a societal, gender or racial nature).</p>
            <p> The released MUSIC-AVQA dataset is curated, which perhaps owns potential correlation between instrument and geographical area. This issue warrants further research and consideration.
            </p>
        </div>
      </div> -->

    <br/>
    <div class="row">
      <div class="col-md-12">
        <h4 class="section-subheading">Copyright <img alt="Creative Commons License" style="border-width:1px;float:left;margin-right:15px;margin-bottom:0px;" src="https://i.creativecommons.org/l/by-nc/3.0/88x31.png"/></h4>
        <p>
          All datasets and benchmarks on this page are copyright by us and published under the <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International</a> License. This means that  you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. You may not use the material for commercial purposes.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- Benchmark -->
<section id="challenges">
  <div class="container">
    
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">A Simple Baseline for Ref-AVS</h2>
        <!-- <h3 class="section-subheading text-muted">Challenge Details with links to &#9733;NEW&#9733; Codalab Leaderboards</h3> -->
        <h3 class="section-subheading text-muted">Audio-visual spatial-temporal Ref-AVS method, experimental results and simple analysis</h3>
      </div>
    </div>
     
    <div class="row">
      <div class="col-md-12">
        <p class="text-muted" style="text-align:left">
          To solve the Ref-AVS problem, we propose a audio-visual grounding model with per-mask segmentation to achieve scene understanding and grounding over audio, visual and language modalities. And to benchmark different models, we use mIoU and F-Score as the evaluation metric. <b>More details in the <a href="{{ site.baseurl }}/static/files/Placeholder.pdf">[Paper]</a> and <a href="{{ site.baseurl }}/static/files/Placeholder.pdf">[Supplementary]</a></b>.<br/>  
        </p> 
      </div>
    </div>
    

    <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Pipeline</h4>
        <p class="text-muted" style="text-align:left">An overview of the proposed framework is illustrated in below figure.
        
        <div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
        <img src="{{ site.baseurl }}/static/img/experiments/framework_pipeline_long.png" style="width: 70%;" class="img-responsive"/> 
        </div>
        <p class="text-muted" style="text-align:left"> Detail can be found in  <a href="{{ site.baseurl }}/static/files/Placeholder.pdf">[Paper]</a> </p>
      </div>
    </div>


    <br/>

    <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Experiments</h4>
        <p class="text-muted" style="text-align:left">
          To study different input modalities and validate the effectiveness of the proposed model, we conduct extensive ablations of our model and compare to recent Refer-VOS and AVS approaches. 
        </p>
        <p class="text-muted" style="text-align:left">
          As shown in <b>right table</b>, we use three test subsets to evaluate the comprehensive ability of Ref-AVS methods. <b>Mix</b> is the average performance of <b>Seen</b> and <b>Unseen</b> test set. We also use <b>Null</b> test set to evaluate the robustness of multimodal-cue expression guidance.
        </p>
        <div class="col-md centered" style="padding:0.6rem; text-align:center">
          <img src="{{ site.baseurl }}/static/img/experiments/exp1.png" style="width: 60%;" class="img-responsive"/> 
        </p>
        <p class="text-muted" style="text-align:left">
          We conduct ablation studies to investigate the impact of two modalities (audio and text) information on the Ref-AVS task, as well as the effectiveness of the proposed method.
        </p>
        <div class="col-md centered" style="padding:0.6rem; text-align:center">
        <img src="{{ site.baseurl }}/static/img/experiments/exp2.png" style="width: 60%;" class="img-responsive"/> 
        </div>
      </div>
    </div>


    <!-- <br/>

    <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Visualized spatio-temporal grounding results</h4>
        <p class="text-muted" style="text-align:left">We provide several visualized spatial grounding results. The heatmap indicates the location of sounding source. Through the spatial grounding results, the sounding objects are visually captured, which can facilitate the spatial reasoning.</p>
        <div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
        <img src="{{ site.baseurl }}/static/img/experiments/visualize1.png" style="width: 100%;" class="img-responsive"/> 
        </div>
        <p class="text-muted" style="text-align:left"><b>Visualized spatio-temporal grounding results</b>. Based on the grounding results of our method, the sounding area and key timestamps are accordingly highlighted in spatial and temporal perspectives <b>(a-e)</b>, respectively, which indicates that our method can model the spatio-temporal association over different modalities well, facilitating the scene understanding and reasoning. Besides, the subfigure <b>(f)</b> shows one failure case predicted by our method, where the complex scenario with multiple sounding and silent objects makes it difficult to correlate individual objects with mixed sound, leading to a wrong answer for the given question.</p>
      </div>
    </div> -->



    

<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat1.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left"><b>Illustrations of our MUSIC-AVQA dataset statistics</b>. <b>(a-d)</b> statistical analysis of the videos and QA pairs. <b>(e)</b> Question formulas. <b>(f)</b> Distribution of question templates, where the dark color indicates the number of QA pairs generated from real videos while the light-colored area on the upper part of each bar means that from synthetic videos. <b>(g)</b> Distribution of first n-grams in questions. Our QA-pairs need <b>fine-grained scene understanding</b> and <b>spatio-temporal reasoning</b> over audio and visual modalities to be solved. For example, existential and location questions require spatial reasoning, and temporal questions require temporal reasoning. Best viewed in color.</p>
    </div> -->
      
   
    
    <!-- <div class="row"> -->
      <!-- <div class="col-md-12"> -->
        <!-- <h4 class="subheading">Challenges/Leaderboard Details</h4>                    

        <p class="text-muted">        
        <b>Splits. </b> The dataset is split in train/validation/test sets, with a ratio of roughly 75/10/15. <br/>         
        The action recognition, detection and anticipation challenges use all the splits. <br/>        
        The unsupservised domain adaptation and action retrieval challenges use different splits as detailed below. <br/>        
        
        You can download all the necessary annotations <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations" target="_blank">here</a>. <br/>
        You can find more details about the splits in <a href="https://arxiv.org/pdf/2006.13256.pdf" target="_blank">our paper</a>.
        </p>
        
        <p class="text-muted">
        <b>Evaluation. </b> All challenges are evaluated considering all segments in the Test split. 
        The action recognition and anticipation challenges are additionally evaluated considering unseen participants and tail classes. These are automatically evaluated in the scripts and you do not need to do anything specific to report these.<br/>
        <b>Unseen participants. </b> The validation and test sets contain participants that are not present in the train set. 
        There are 2 unseen participants in the validation set, and another 3 participants in the test set. 
        The corresponding action segments are 1,065 and 4,110 respectively. <br/>
        <b>Tail classes. </b> These are the set of smallest classes whose instances account for 20&#37 of the total number of instances in 
        training. A tail action class contains either a tail verb class or a tail noun class. 
        <br/><br/>
        </p> -->
        
        
        
        <!-- <section class="challenge" id="challenge-action-detection">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Action Detection</h5>
              <p class="text-muted">
                <b>Task. </b> 
                Detect the start and the end of each action in an <i>untrimmed</i> video. Assign a (verb, noun) label to each 
                detected segment. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label.  <br/>                
                <b>Testing input. </b> A set of <i>untrimmed</i> videos. <u>Important:</u> You are not allowed to use the knowledge of trimmed segments in the test set when reporting for this challenge.<br/>
                <b>Splits. </b> Train and validation for training, evaluated on the test split. <br/>
                <b>Evaluation metrics. </b> Mean Average Precision (mAP) @ IOU 0.1 to 0.5.
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C2-Action-Detection">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C2-Action-Detection">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/707#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/707#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/707#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/707#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/ad.png" style="width: 100%" class="img-responsive"/> 
         </figure> 
        </section> -->
        

        <!-- <section class="challenge" id="challenge-action-anticipation">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Action Anticipation</h5>
              <p class="text-muted">
                <b>Task. </b> 
                Predict the (verb, noun) label of a future action observing a segment preceding its occurrence. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label. <br/>
                <b>Testing input. </b> During testing you are allowed to observe a segment that <i>ends</i> at least one second before 
                the start of the action you are testing on.<br/>                
                <b>Splits. </b> Train and validation for training, evaluated on the test split. <br/>
                <b>Evaluation metrics. </b> Top-5 recall averaged for all classes, as defined <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Furnari_Leveraging_Uncertainty_to_Rethink_Loss_Functions_and_Evaluation_Measures_for_ECCVW_2018_paper.pdf" target="_blank">here</a>,
                calculated for all segments as well as unseen participants and tail classes.
                <br/>
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C3-Action-Anticipation">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C3-Action-Anticipation">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/702#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/702#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/702#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/702#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/aa.png" style="width: 100%" class="img-responsive"/> 
         </figure>
        </section> -->
        
        <!-- <section class="challenge" id="challenge-domain-adaptation">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Unsupervised Domain Adaptation for Action Recognition</h5>
              <p class="text-muted">
                <b>Task. </b> Assign a (verb, noun) label to a trimmed segment, following the Unsupervised Domain Adaptation paradigm: 
                a labelled source domain is used for training, and the model needs to adapt to an unlabelled target domain. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label.  <br/>                
                <b>Testing input. </b> A set of trimmed unlabelled action segments. <br/>
                <b>Splits. </b> Videos recorded in 2018 (EPIC-KITCHENS-55) constitute the source domain, 
                while videos recorded for MUSIC-AVQA's extension constitute the unlabelled target domain. 
                This challenge uses custom train/validation/test splits, which you can find 
                <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations#unsupervised-domain-adaptation-challenge" target="_blank">here</a>. <br/> 
                <b>Evaluation metrics. </b> Top-1/5 accuracy for verb, noun and action (verb+noun), on the target test set.
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C4-UDA-for-Action-Recognition">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C4-UDA-for-Action-Recognition">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/uda.png" style="width: 100%" class="img-responsive"/> 
         </figure>
        </section>-->


        <!-- <section class="challenge" id="challenge-action-retrieval">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Multi-Instance Retrieval</h5>
              <p class="text-muted">
                <b>Tasks. </b> <i>Video to text</i>: given a query video segment, rank captions such that those with a higher rank are 
                more semantically relevant to the action in the query video segment. 
                <i>Text to video:</i> given a query caption, rank video segments such that those with a higher rank are more semantically relevant 
                to the query caption. <br/>                                
                <b>Training input. </b> A set of trimmed action segments, each annotated with a caption. 
                Captions correspond to the narration in English from which the action segment was obtained. <br/>                
                <b>Testing input. </b> A set of trimmed action segments with captions. Important: You are not allowed to use the known correspondence in the Test set <br/>
                <b>Splits. </b> This challenge has its own custom splits, available <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations/tree/master/retrieval_annotations">here</a>. <br/>                
                <b>Evaluation metrics. </b> normalised Discounted Cumulative Gain (nDCG) and Mean Average Precision (mAP). 
                You can find more details in <a href="https://arxiv.org/pdf/2006.13256.pdf" target="_blank">our paper</a>. 
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C5-Multi-Instance-Retrieval">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C5-Multi-Instance-Retrieval">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/617#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/617#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/617#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/617#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/aret.png" style="width: 100%" class="img-responsive"/> 
         </figure>
          
        </section> -->
        
      <!-- </div>
    </div> -->
    
  </div>
</section> 

<!-- Team -->
<section class="bg-light" id="team">
  <div class="container">
    
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">The Team</h2>
        <div class="text-muted">
          <p> We are a group of researchers working in computer vision
            from the <a href="http://ai.ruc.edu.cn/">Renmin University of China</a> and <a href="https://www.bupt.edu.cn">Beijing University of Posts and Telecommunictaions</a>.
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-md-6">
        <a href="http://ai.ruc.edu.cn/">
          <img src="{{ site.baseurl }}/static/img/universities/ruc.png" alt="" style="width:80%;  margin-top:10px; margin-bottom:15px;">
        </a>
      </div>
      <div class="col-md-6">
        <a href="https://www.bupt.edu.cn">
          <img src="{{ site.baseurl }}/static/img/universities/bupt.png" alt="" style="width:80%;  margin-top:29px; margin-bottom:15px;">
        </a>
      </div>
    </div>
    <hr>



<!--<div class="row">
      <div class="col-md-6">
        <div class="team-member">
          <a href="http://dimadamen.github.io">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/dd-min.jpg"/>

            <h4>Guangyao Li</h4></a>
            <h5>Renmin University of China</h5>
            <h6 class="text-muted">University of Bristol, United Kingom</h6>
        </div>
      </div>
        
      <div class="col-md-6">
        <div class="team-member">
          <a href="http://www.dmi.unict.it/farinella/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/gmf-min.jpg" />

            <h4>Giovanni Maria Farinella</h4></a>
          <h5>Co-Investigator</h5>
          <h6 class="text-muted">University of Catania, Italy</h6>
        </div>
      </div>
    </div>
-->


    <div class="container">
      <div class="row">

<!--         <div class="col-lg-12">
          <h2 class="section-heading text-uppercase">Research Funding</h2>
          <div class="text-muted">
            <p> The work on MUSIC-AVQA was supported by the following research grants</p>
              <ul class="text-muted">
                <li>Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China.</li>
                <li>Beijing Outstanding Young Scientist Program (NO.BJJWZYJH012019100020098)</li>
                <li>Research Funds of Renmin University of China (NO.21XNLG17)</li>
                <li>National Natural Science Foundation of China (NO.62106272)</li>
                <li>2021 Tencent AI Lab Rhino-Bird Focused Research Program (No.JR202141)</li>
                <li>Young Elite Scientists Sponsorship Program by CAST</li>
                <li>Large-Scale Pre-Training Program of Beijing Academy of Artificial Intelligence (BAAI)</li>
              </ul>
          </div>
        </div> -->

        <div class="col-lg-12">
          <h2 class="section-heading text-uppercase">Acknowledgement</h2>
          <div class="text-muted">
              <ul class="text-muted">
                <li>This research was supported by Public Computing Cloud, Renmin University of China.</li>
                <li>This web-page design inspired by EPIC official website.</li>
              </ul>
          </div>
        </div>

      </div>
    </div>
</section>


<!--<section class="bg-light" id="results">
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">Results - 2021 Challenges (June 2021)</h2>
        <div class="text-muted">
            <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">EPIC-Kitchens Challenges @CVPR2021, Virtual CVPR</h4>
        <div class="row">
           <div class="col-md-3">
                Jan 1, 2021
            </div>
            <div class="col-md-9">
                EPIC-Kitchens Challenges 2020 Launched!
            </div>
        </div>
        <div class="row">
            <div class="col-md-3">
                June 1, 2021
            </div>
            <div class="col-md-9">
                Server Submission Deadline at 23:59:59 GMT
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                Jun 4, 2020
            </div>
            <div class="col-md-9">
                Deadline for Submission of Technical Reports
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                June 20, 2020
            </div>
            <div class="col-md-9">
                Results announced at <a href="https://eyewear-computing.org/EPIC_CVPR21/">EPIC@CVPR2021</a> Workshop (<a href="https://youtu.be/FSn8yCbpcc4">watch session recording here</a>)
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                July 6, 2021
            </div>
            <div class="col-md-9">
                Technical report for all submissions to the 2021 challenges is now <a href="Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">available here</a> [Reference <a href="./Reports/2021-bibtex.txt">Bibtex</a>].
            </div>
        </div>  
      </div>
    </div>
            
        <h2 class="section-heading text-uppercase">2021 Challenge Winners</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/winnersList-2021.png" width=100%/>
                  </div>
                   <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/winners-2021.png" width=100%/>
                  </div>
            
          <h2 class="section-heading text-uppercase">Action Recognition Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AR.png" width=100%/>
                  </div>
           <h2 class="section-heading text-uppercase">Action Anticipation Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AN.png" width=100%/>
                  </div>
          <h2 class="section-heading text-uppercase">Action Detection Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AD.png" width=100%/>
                  </div>
             <h2 class="section-heading text-uppercase">Unsupervised Domain Adaptation for Action Recognition Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/UDA.png" width=100%/>
                  </div>
            
             <h2 class="section-heading text-uppercase">Multi-Instance Retrieval Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/MIR.png" width=100%/>
                  </div>
        </div>
      </div>
    </div>
    </div>
</section>-->

<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"MUSIC-AVQA dataset",
  "description":"First-person (egocentric) video dataset; multi-faceted non-scripted recordings in the wearers' homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel live audio commentary approach.",
  "url":"https://github.com/epic-kitchens/annotations",
  "sameAs":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d",
  "citation":"Damen, Dima et al. 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset', European Conference on Computer Vision, 2018",
  "identifier": "10.5523/bris.3h91syskeag572hl6tvuovwv4d",
  "keywords":[
     "Egocentric vision",
     "Human actions",
     "Object interactions",
     "actions",
     "video",
     "kitchens",
     "cooking",
     "dataset",
     "epic kitchens",
     "epic",
     "eccv",
     "2022"
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://epic-kitchens.github.io/",
     "name":"EPIC Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"uob-epic-kitchens@bristol.ac.uk",
        "url":"https://github.com/epic-kitchens/annotations/issues"
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     }
  ],
  "license": "https://creativecommons.org/licenses/by-nc/4.0/"
}
</script>

